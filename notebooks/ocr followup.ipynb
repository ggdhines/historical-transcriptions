{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import copy\n",
    "import re\n",
    "from sklearn import neighbors\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sqlalchemy as db\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "# https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='historical-transcriptions' user='ghines' host='localhost' password='123456'\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"select * from pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'psycopg2.extensions.connection'>\n",
      "<class 'sqlalchemy.engine.base.Engine'>\n"
     ]
    }
   ],
   "source": [
    "print(type(conn))\n",
    "print(type(engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/ggdhines/bear/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_character(character_df,index):\n",
    "    row = character_df.loc[index]\n",
    "    img = cv2.imread(directory+row[\"fname\"])\n",
    "\n",
    "    tile = img[row.top:row.bottom,row.left:row.right]\n",
    "\n",
    "    _ = plt.imshow(tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bear-AG-29-1940-01-49_ocr_ready.png\r"
     ]
    }
   ],
   "source": [
    "tile_df = []\n",
    "tiles = []\n",
    "\n",
    "master_list = [chr(ord('0')+i) for i in range(10)]\n",
    "\n",
    "for fname in os.listdir(directory):\n",
    "    match = re.search(\"(.*)-(\\d+)-(\\d+)\\-(\\d*)_ocr_ready.png\",fname)\n",
    "    if match is None:\n",
    "        continue\n",
    "                \n",
    "    print(fname,end=\"\\r\")\n",
    "        \n",
    "    csv_file = fname[:-13] + \"ocr.csv\"\n",
    "    img = cv2.imread(directory+fname,0)\n",
    "    \n",
    "    tiles_on_page = pd.read_csv(directory+csv_file,delimiter=\" \",error_bad_lines=False, engine=\"python\",quoting=3)\n",
    "    \n",
    "#     m1 = tiles[\"confidence\"] > 95    \n",
    "#     tiles = tiles[m1]\n",
    "    \n",
    "    max_darkness = []\n",
    "    for _,row in tiles_on_page.iterrows():\n",
    "        tile = img[row.top:row.bottom,row.left:row.right]\n",
    "        resized_tile = cv2.resize(tile,(28,28))\n",
    "        max_darkness.append(np.min(tile))\n",
    "        tiles.append(resized_tile)\n",
    "        \n",
    "    tiles_on_page[\"darkest_pixel\"] = max_darkness\n",
    "#     df[\"fname\"] = fname\n",
    "    \n",
    "    try:\n",
    "        tiles_on_page[\"ship_name\"] = match.groups()[0]\n",
    "        tiles_on_page[\"year\"] = int(match.groups()[1])\n",
    "        tiles_on_page[\"month\"] = int(match.groups()[2])\n",
    "        tiles_on_page[\"page_number\"] = int(match.groups()[3])\n",
    "    except ValueError:\n",
    "        print(match.groups())\n",
    "        raise\n",
    "    \n",
    "    tile_df.append(tiles_on_page)\n",
    "    \n",
    "tile_df = pd.concat(tile_df,ignore_index=True)\n",
    "tile_df[\"area\"] = (tile_df[\"right\"] - tile_df[\"left\"]) * (tile_df[\"bottom\"] - tile_df[\"top\"])\n",
    "tiles = np.asarray(tiles)\n",
    "s = tiles.shape\n",
    "tiles = tiles.reshape((s[0],s[1],s[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine('postgres://ghines:123456@127.0.0.1:5432/historical-transcriptions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter in any newly added pages and get the page index for all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ship_name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ship_name  year  month  page_number  page_id\n",
       "0  Bear-AG-29  1940      1           11       32\n",
       "1  Bear-AG-29  1940      1           21       33\n",
       "2  Bear-AG-29  1940      1           63       34\n",
       "3  Bear-AG-29  1940      1           25       35\n",
       "4  Bear-AG-29  1940      1           13       36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ship_name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ship_name, year, month, page_number, page_id]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['character', 'confidence', 'left', 'top', 'right', 'bottom',\n",
      "       'darkest_pixel', 'ship_name', 'year', 'month', 'page_number', 'area',\n",
      "       'page_id_x', 'language_model', 'index_wrt_lang_model', 'page_id_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns = [\"ship_name\",\"year\",\"month\",\"page_number\"]\n",
    "pages_df = tile_df[columns].drop_duplicates()\n",
    "\n",
    "already_entered_pages = pd.read_sql(\"pages\",engine)\n",
    "\n",
    "display.display(already_entered_pages.head())\n",
    "\n",
    "\n",
    "to_add = pages_df.merge(already_entered_pages,how=\"left\",on=columns)\n",
    "to_add = to_add[to_add[\"page_id\"].isna()]\n",
    "\n",
    "display.display(to_add)\n",
    "to_add[columns].to_sql(\"pages\",engine,if_exists=\"append\",index=False)\n",
    "\n",
    "all_pages = pd.read_sql(\"pages\",engine)\n",
    "\n",
    "tile_df = tile_df.merge(all_pages,on=columns)\n",
    "\n",
    "print(tile_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, write tile information to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tile_df[\"language_model\"] = \"eng\"\n",
    "tile_df[\"index_wrt_lang_model\"] = tile_df.index\n",
    "\n",
    "columns = [\"language_model\",\n",
    "           \"index_wrt_lang_model\",\n",
    "           \"character\",\n",
    "           \"confidence\",\n",
    "           \"top\",\n",
    "           \"left\",\n",
    "           \"right\",\n",
    "           \"bottom\",\n",
    "           \"darkest_pixel\",\n",
    "           \"area\",\n",
    "           \"page_id\"]\n",
    "\n",
    "print(tile_df[columns].to_sql(\"tesseractResults\",engine,if_exists=\"append\",index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_model</th>\n",
       "      <th>index_wrt_lang_model</th>\n",
       "      <th>character</th>\n",
       "      <th>confidence</th>\n",
       "      <th>top</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>bottom</th>\n",
       "      <th>darkest_pixel</th>\n",
       "      <th>area</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>83.294563</td>\n",
       "      <td>1252</td>\n",
       "      <td>932</td>\n",
       "      <td>951</td>\n",
       "      <td>1270</td>\n",
       "      <td>160</td>\n",
       "      <td>342</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>1</td>\n",
       "      <td>|</td>\n",
       "      <td>92.035088</td>\n",
       "      <td>1306</td>\n",
       "      <td>1153</td>\n",
       "      <td>1156</td>\n",
       "      <td>1309</td>\n",
       "      <td>196</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td>o</td>\n",
       "      <td>86.671074</td>\n",
       "      <td>1271</td>\n",
       "      <td>1828</td>\n",
       "      <td>1830</td>\n",
       "      <td>1273</td>\n",
       "      <td>198</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng</td>\n",
       "      <td>3</td>\n",
       "      <td>o</td>\n",
       "      <td>87.580971</td>\n",
       "      <td>1269</td>\n",
       "      <td>1864</td>\n",
       "      <td>1878</td>\n",
       "      <td>1289</td>\n",
       "      <td>185</td>\n",
       "      <td>280</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td>_</td>\n",
       "      <td>85.450989</td>\n",
       "      <td>1291</td>\n",
       "      <td>3048</td>\n",
       "      <td>3053</td>\n",
       "      <td>1296</td>\n",
       "      <td>191</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41481</th>\n",
       "      <td>eng</td>\n",
       "      <td>41481</td>\n",
       "      <td>e</td>\n",
       "      <td>89.111053</td>\n",
       "      <td>11080</td>\n",
       "      <td>6769</td>\n",
       "      <td>6870</td>\n",
       "      <td>11082</td>\n",
       "      <td>200</td>\n",
       "      <td>202</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41482</th>\n",
       "      <td>eng</td>\n",
       "      <td>41482</td>\n",
       "      <td>e</td>\n",
       "      <td>89.210991</td>\n",
       "      <td>11062</td>\n",
       "      <td>6961</td>\n",
       "      <td>7000</td>\n",
       "      <td>11082</td>\n",
       "      <td>199</td>\n",
       "      <td>780</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41483</th>\n",
       "      <td>eng</td>\n",
       "      <td>41483</td>\n",
       "      <td>o</td>\n",
       "      <td>90.023804</td>\n",
       "      <td>11046</td>\n",
       "      <td>7305</td>\n",
       "      <td>7362</td>\n",
       "      <td>11088</td>\n",
       "      <td>197</td>\n",
       "      <td>2394</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41484</th>\n",
       "      <td>eng</td>\n",
       "      <td>41484</td>\n",
       "      <td>t</td>\n",
       "      <td>92.318863</td>\n",
       "      <td>11002</td>\n",
       "      <td>7332</td>\n",
       "      <td>7454</td>\n",
       "      <td>11085</td>\n",
       "      <td>193</td>\n",
       "      <td>10126</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41485</th>\n",
       "      <td>eng</td>\n",
       "      <td>41485</td>\n",
       "      <td>)</td>\n",
       "      <td>92.338936</td>\n",
       "      <td>10991</td>\n",
       "      <td>7442</td>\n",
       "      <td>7472</td>\n",
       "      <td>11083</td>\n",
       "      <td>182</td>\n",
       "      <td>2760</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41486 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      language_model  index_wrt_lang_model character  confidence    top  left  \\\n",
       "0                eng                     0         .   83.294563   1252   932   \n",
       "1                eng                     1         |   92.035088   1306  1153   \n",
       "2                eng                     2         o   86.671074   1271  1828   \n",
       "3                eng                     3         o   87.580971   1269  1864   \n",
       "4                eng                     4         _   85.450989   1291  3048   \n",
       "...              ...                   ...       ...         ...    ...   ...   \n",
       "41481            eng                 41481         e   89.111053  11080  6769   \n",
       "41482            eng                 41482         e   89.210991  11062  6961   \n",
       "41483            eng                 41483         o   90.023804  11046  7305   \n",
       "41484            eng                 41484         t   92.318863  11002  7332   \n",
       "41485            eng                 41485         )   92.338936  10991  7442   \n",
       "\n",
       "       right  bottom  darkest_pixel   area  page_id  \n",
       "0        951    1270            160    342       32  \n",
       "1       1156    1309            196      9       32  \n",
       "2       1830    1273            198      4       32  \n",
       "3       1878    1289            185    280       32  \n",
       "4       3053    1296            191     25       32  \n",
       "...      ...     ...            ...    ...      ...  \n",
       "41481   6870   11082            200    202       62  \n",
       "41482   7000   11082            199    780       62  \n",
       "41483   7362   11088            197   2394       62  \n",
       "41484   7454   11085            193  10126       62  \n",
       "41485   7472   11083            182   2760       62  \n",
       "\n",
       "[41486 rows x 11 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(\"tessearctResults\",engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ink pixels are fairly bright (barely indistinguishable from paper pixels), and so we set our threshold for paper/ink fairly high. However, for a full character, there should be at least some dark pixels, i.e. the bright ink pixels are on the boundary between ink and paper. The interior of the character will be dark. So if Tesseract finds a character where every pixel is bright, we should be worried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(all_characters[\"max_darkness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "all_characters.plot.scatter(x=\"max_darkness\",y=\"area\",ax=ax)\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = all_characters[\"max_darkness\"] <= 170\n",
    "all_characters[m].sort_values(\"max_darkness\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of these characters are completely suspect, but Tesseract still feels fairly confident. Also, for most the area seems far too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(all_characters,6771)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play around with the threshold and see that requiring max_darkness <= 170 seems reasonable. (Might be nice to create a more automated filter in the future.) From below, we see that this filters out about 7% of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darkness_mask = all_characters[\"max_darkness\"] <= 170\n",
    "s1 = all_characters[darkness_mask].shape[0]\n",
    "s2 = all_characters.shape[0]\n",
    "print(s1/s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, characters which are way too big or small need to be examined. Filtering out impossibly small characters is straightforward. However, it is a bit more complicated with bigger characters. These often include the correct character plus a whole bunch more. We will filter them out for now, since all of this is going to be fed into the autoencoder which benefits from characters being as similar as possible. However, we will want to feed these \"characters\" through the website to have the bounding boxes correct, whereas we can just drop the overly small characters completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_characters = all_characters[darkness_mask]\n",
    "filtered_characters.sort_values(\"area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(all_characters,14674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "bins = range(filtered_characters[\"area\"].min(),filtered_characters[\"area\"].max(),1000)\n",
    "filtered_characters[\"area\"].hist(bins=bins)\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = filtered_characters[\"area\"] <= 10000\n",
    "filtered_characters[m].sort_values(\"area\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(all_characters,24215)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An upper bound of 1000 for area seems good, but we could probably reduce it a bit too if we wanted to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = filtered_characters[\"area\"] > 2000\n",
    "filtered_characters[m].sort_values(\"area\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounding box below is definitely too small, but the character is correct. So we may need to follow up on such bounding boxes in the website. (Also check to see if we have overlapping bounding boxes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(all_characters,12152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = filtered_characters[\"area\"] <= 10000\n",
    "m2 = filtered_characters[\"area\"] >= 2000\n",
    "\n",
    "double_filtered_characters = filtered_characters[m1&m2]\n",
    "s1 = double_filtered_characters.shape[0]\n",
    "s2 = all_characters.shape[0]\n",
    "print(s1/s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(character_df,tiles):\n",
    "    train_df,test_df = train_test_split(character_df, test_size=0.25, random_state=0)\n",
    "\n",
    "    train_images = tiles[train_df.index]\n",
    "    # we no longer need this index wrt to the original dataframe\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    test_images = tiles[test_df.index]\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    train_images = train_images / 255\n",
    "    test_images = test_images / 255\n",
    "\n",
    "    # sanity check\n",
    "    assert np.max(train_images) == 1\n",
    "    return train_images,test_images,train_df,test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,test_images,train_df,test_df = split(double_filtered_characters,tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display(train_df.iloc[0])\n",
    "_ = plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is taken from https://www.tensorflow.org/tutorials/generative/cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                # No activation\n",
    "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "                tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "\n",
    "    # not sure why the following had to be added in\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, epoch, test_sample):\n",
    "    mean, logvar = model.encode(test_sample)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    predictions = model.sample(z)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "#     plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_images,test_images,model_name,epochs):\n",
    "    # set the dimensionality of the latent space to a plane for visualization later\n",
    "    latent_dim = 2\n",
    "    num_examples_to_generate = 16\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_size = train_images.shape[0]\n",
    "    test_size = test_images.shape[0]\n",
    "\n",
    "\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
    "                     .shuffle(train_size).batch(batch_size))\n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "                    .shuffle(test_size).batch(batch_size))\n",
    "\n",
    "    assert batch_size >= num_examples_to_generate\n",
    "    for test_batch in test_dataset.take(1):\n",
    "        test_sample = test_batch[0:num_examples_to_generate, :, :, :]\n",
    "    \n",
    "    # keeping the random vector constant for generation (prediction) so\n",
    "    # it will be easier to see the improvement.\n",
    "    random_vector_for_generation = tf.random.normal(\n",
    "        shape=[num_examples_to_generate, latent_dim])\n",
    "    model = CVAE(latent_dim)\n",
    "\n",
    "    generate_images(model, 0, test_sample)\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        for train_x in train_dataset:\n",
    "            train_step(model, train_x, optimizer)\n",
    "        end_time = time.time()\n",
    "\n",
    "        loss = tf.keras.metrics.Mean()\n",
    "        for test_x in test_dataset:\n",
    "            loss(compute_loss(model, test_x))\n",
    "        elbo = -loss.result()\n",
    "\n",
    "\n",
    "        display.clear_output(wait=False)\n",
    "        print('Epoch: {}, Test set ELBO: {},time elapse for current epoch: {}'\n",
    "            .format(epoch, elbo,end_time - start_time))\n",
    "        generate_and_save_images(model, epoch, test_sample)\n",
    "        \n",
    "        \n",
    "            \n",
    "    return model\n",
    "\n",
    "def load_or_train_model(train_images,test_images,model_name,epochs):\n",
    "    \"\"\"\n",
    "    load a previously trained model. If no such model exists, train one\n",
    "    \"\"\"\n",
    "    weights_file = f\"{directory}weights_{model_name}\"\n",
    "    \n",
    "    if not os.path.exists(weights_file+\".index\"):\n",
    "        model = train_model(train_images,test_images,model_name,epochs)\n",
    "        model.save_weights(weights_file)\n",
    "    else:\n",
    "        print(\"loading\")\n",
    "        latent_dim = 2\n",
    "        num_examples_to_generate = 16\n",
    "        batch_size = 64\n",
    "        \n",
    "        model = CVAE(latent_dim)\n",
    "        model.load_weights(weights_file)\n",
    "        \n",
    "        # show how the loaded model is doing\n",
    "        test_size = test_images.shape[0]\n",
    "        test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "                    .shuffle(test_size).batch(batch_size))\n",
    "        for test_batch in test_dataset.take(1):\n",
    "            test_sample = test_batch[0:num_examples_to_generate, :, :, :]\n",
    "            \n",
    "        generate_images(model, 0, test_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very biased data set, so we'll check to see if reweighting the training and test set improves things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_model = load_or_train_model(train_images,test_images,\"base\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our CVAE, we will take a test example and see how well the autoencoder corrects the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = double_filtered_characters.groupby(\"character\").sample(n=150,replace=True)\n",
    "resampled_train_images,resampled_test_images,resampled_train_df,resampled_test_df = split(resampled,tiles)\n",
    "resampled_model = load_or_train_model(resampled_train_images,resampled_test_images,\"resampled\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could always increase the sameple size for the reweighted train/test set, but the initial results shown below are not promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df[\"character\"] == \"8\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df[\"character\"] == \"N\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 24\n",
    "\n",
    "print(test_df.loc[index,\"character\"])\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(test_images[index,:,:,0])\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "a,b = base_model.encode(test_images[index:index+1,:,:,:])\n",
    "z = base_model.reparameterize(a,b)\n",
    "x = base_model.decode(z,apply_sigmoid=True)\n",
    "ax.imshow(x[0,:,:,0])\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "a,b = resampled_model.encode(test_images[index:index+1,:,:,:])\n",
    "z = resampled_model.reparameterize(a,b)\n",
    "x = resampled_model.decode(z,apply_sigmoid=True)\n",
    "ax.imshow(x[0,:,:,0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well is our CVAE able to separate the characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characters_to_latent(df,images,model):\n",
    "    assert isinstance(df,pd.DataFrame)\n",
    "    mu = []\n",
    "    sigma = []\n",
    "\n",
    "    for index in range(images.shape[0]):\n",
    "        print(index,end=\"\\r\")\n",
    "\n",
    "        original = images[index:index+1,:,:,:]\n",
    "\n",
    "        a,b = model.encode(original)\n",
    "        z = model.reparameterize(a,b)\n",
    "\n",
    "        mu.append(float(z[0][0]))\n",
    "        sigma.append(float(z[0][1]))\n",
    "\n",
    "    df2 = pd.DataFrame({\"mu\":mu,\"sigma\":sigma})\n",
    "    return pd.concat([df,df2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_latent_df = characters_to_latent(resampled_test_df,resampled_test_images,resampled_model)\n",
    "latent_df = characters_to_latent(test_df,test_images,base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick test, how well does our CVAE differentiate numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = [chr(ord('0') + i) for i in range(10)]\n",
    "m = latent_df[\"character\"].isin(of_interest)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='mu', y='sigma', hue='character', data=latent_df[m])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(10, weights='uniform')\n",
    "clf.fit(latent_df[[\"mu\",\"sigma\"]], latent_df[\"character\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df = latent_df.groupby(\"character\")[[\"mu\",\"sigma\"]].median().reset_index()\n",
    "ideal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code seems to be easier to follow if everything is in dense format (as opposed to sparse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "ideal_as_1hot = enc.fit_transform(ideal_df[[\"character\"]]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = clf.predict_proba(ideal_df[[\"mu\",\"sigma\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely = np.multiply(ideal_as_1hot,probabilities)\n",
    "ideal_df[\"likelyhood\"] = np.amax(most_likely,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which characters are we most confident about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df.sort_values(\"likelyhood\").tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_character(model,x,y):\n",
    "    s = np.array([x,y])\n",
    "    s = s.reshape(1,2)\n",
    "    \n",
    "    remapped = model.decode(s,apply_sigmoid=True)\n",
    "\n",
    "    a = (np.array(remapped)*255).astype(np.uint8)\n",
    "    b = a.reshape((a.shape[1],a.shape[2]))\n",
    "    ret2,th2 = cv2.threshold(b,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    th2 = 255 - th2\n",
    "    return th2\n",
    "\n",
    "def generate_ideal(model,ideal_df,ch):\n",
    "    r = ideal_df[ideal_df[\"character\"] == ch]\n",
    "    return generate_character(model,r[\"mu\"],r[\"sigma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CVAE handles '4' very well. We have identified some characters which according to our model are 4 but Tessearct thinks differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(generate_ideal(base_model,ideal_df,\"T\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at some of the raw characters. Start with characters we think are likely to be 4, but tesseract doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df[ideal_df[\"character\"] == \"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = enc.transform(latent_df[[\"character\"]]).todense()\n",
    "probabilities = clf.predict_proba(latent_df[[\"mu\",\"sigma\"]])\n",
    "latent_df[\"p\"] = probabilities[:,15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when Tessearct thinks a character is 4, 77% of the time we agree. However, when Tessearct thinks a character is k, 60% we think the character is actaully 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.groupby(\"character\")[\"p\"].mean().to_frame().sort_values(\"p\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = results_df[\"character\"] == \"k\"\n",
    "results_df[m].sort_values(\"p\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"/home/ggdhines/bear/Bear-AG-29-1940-01-39_ocr_ready.png\",0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = results_df[\"fname\"] == \"Bear-AG-29-1940-01-39_ocr_ready.png\"\n",
    "m2  = results_df[\"max prob\"] >= 0.8\n",
    "results_df[m1].sort_values(\"top\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our classifcation is better than Tessearct's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(results_df,4159)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Tesseract is actually often mislabelling '4's as 'k's. Note that the above 4 is missing a fair bit since it overlaps with a grid line, yet we still estimate the probability of it being a '4' to be 83%/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size vs. likelyhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = results_df.groupby(\"character\").size().reset_index()\n",
    "df3 = ideal_df.merge(df2,on=\"character\")\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "df3.plot.scatter(x=0,y=\"likelyhood\",ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is surprisingly little correlation between Tessearct's confidence and ours\n",
    "### (Remeber that we filtered to only includes tiles which Tesseract had a confidence of at least 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual = enc.transform(latent_df[[\"character\"]]).todense()\n",
    "probabilities = clf.predict_proba(latent_df[[\"mu\",\"sigma\"]])\n",
    "latent_df[\"max prob\"] = np.amax(probabilities,axis=1)\n",
    "\n",
    "_ = latent_df.plot.scatter(x=\"confidence\",y=\"max prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Tiles are we least certain about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely = np.argmax(probabilities,axis=1)\n",
    "inverse = [c[-1] for c in enc.get_feature_names()]\n",
    "most_likely = [inverse[i] for i in most_likely]\n",
    "latent_df[\"most_likely\"] = most_likely\n",
    "\n",
    "latent_df.sort_values(\"max prob\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here Tesseract is completely correct and we're not. So what happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.to_csv(\"/home/ggdhines/PycharmProjects/historical-transcriptions/dataframes/latent1.cvs\")\n",
    "ideal_df.to_csv(\"/home/ggdhines/PycharmProjects/historical-transcriptions/dataframes/ideal.cvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(latent_df,582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df[ideal_df[\"character\"].isin([\"N\",\"\\\"\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = [\"N\",\"\\\"\"]\n",
    "m = latent_df[\"character\"].isin(of_interest)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='mu', y='sigma', hue='character', data=latent_df[m])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ideal_df.loc[ideal_df[\"character\"] == \"2\",[\"mu\",\"sigma\"]].values[0]\n",
    "\n",
    "m = results_df[\"character\"] == \"2\"\n",
    "df2 = results_df[m]\n",
    "d = df2[[\"mu\",\"sigma\"]].values\n",
    "\n",
    "distance = np.sqrt((d[:,0]-u[0])**2 + (d[:,1]-u[1])**2)\n",
    "df2[\"distance\"] = distance\n",
    "\n",
    "df2.sort_values(\"distance\",ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(test_images[1614,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So why is Z so bad? Relatively few tiles. How good are they?\n",
    "\n",
    "* with less than 100 tiles, knn will underestimate the likelyhood of Z. But the CVAE seems to have a hard differentiating Z from E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = results_df[\"character\"] == \"Z\"\n",
    "results_df[m].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(test_images[3627,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two tiles. Both have low max probability. Tesseract identifies the first as being for a character which we have a high confidence for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df.sort_values(\"likelyhood\",ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = latent_df[\"character\"] == \"%\"\n",
    "latent_df[m].sort_values(\"max prob\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(test_images[3066,:,:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
