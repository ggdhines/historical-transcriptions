{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import copy\n",
    "import re\n",
    "from sklearn import neighbors\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sqlalchemy as db\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "# https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/ggdhines/bear/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_character(character_df,index):\n",
    "    row = character_df.loc[index]\n",
    "    img = cv2.imread(directory+row[\"fname\"])\n",
    "\n",
    "    tile = img[row.top:row.bottom,row.left:row.right]\n",
    "\n",
    "    _ = plt.imshow(tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bear-AG-29-1940-01-49_ocr_ready.png\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8201e618749f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mall_characters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtile_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mtile_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"area\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtile_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtile_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtile_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bottom\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtile_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"top\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_8/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_8/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 )\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "tile_df = []\n",
    "tile\n",
    "master_list = [chr(ord('0')+i) for i in range(10)]\n",
    "\n",
    "for fname in os.listdir(directory):\n",
    "    match = re.search(\"(.*)-(\\d+)-(\\d+)\\-(\\d*)_ocr_ready.png\",fname)\n",
    "    if match is None:\n",
    "        continue\n",
    "                \n",
    "    print(fname,end=\"\\r\")\n",
    "        \n",
    "    csv_file = fname[:-13] + \"ocr.csv\"\n",
    "    img = cv2.imread(directory+fname,0)\n",
    "    \n",
    "    tiles_on_page = pd.read_csv(directory+csv_file,delimiter=\" \",error_bad_lines=False, engine=\"python\",quoting=3)\n",
    "    \n",
    "#     m1 = tiles[\"confidence\"] > 95    \n",
    "#     tiles = tiles[m1]\n",
    "    \n",
    "    max_darkness = []\n",
    "    for _,row in tiles_on_page.iterrows():\n",
    "        tile = img[row.top:row.bottom,row.left:row.right]\n",
    "        resized_tile = cv2.resize(tile,(28,28))\n",
    "        max_darkness.append(np.min(tile))\n",
    "        tiles.append(resized_tile)\n",
    "        \n",
    "    tiles_on_page[\"max_darkness\"] = max_darkness\n",
    "#     df[\"fname\"] = fname\n",
    "    \n",
    "    try:\n",
    "        tiles_on_page[\"ship_name\"] = match.groups()[0]\n",
    "        tiles_on_page[\"year\"] = int(match.groups()[1])\n",
    "        tiles_on_page[\"month\"] = int(match.groups()[2])\n",
    "        tiles_on_page[\"page_number\"] = int(match.groups()[3])\n",
    "    except ValueError:\n",
    "        print(match.groups())\n",
    "        raise\n",
    "    \n",
    "    all_characters.append(df)\n",
    "    \n",
    "tile_df = pd.concat(tiles,ignore_index=True)\n",
    "tile_df[\"area\"] = (tile_df[\"right\"] - tile_df[\"left\"]) * (tile_df[\"bottom\"] - tile_df[\"top\"])\n",
    "tiles = np.asarray(tiles)\n",
    "s = tiles.shape\n",
    "tiles = tiles.reshape((s[0],s[1],s[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine('postgres://ghines:123456@127.0.0.1:5432/historical-transcriptions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter in any newly added pages and get the page index for all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ship_name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bear-AG-29</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ship_name  year  month  page_number  page_id\n",
       "0  Bear-AG-29  1940      1           11       32\n",
       "1  Bear-AG-29  1940      1           21       33\n",
       "2  Bear-AG-29  1940      1           63       34\n",
       "3  Bear-AG-29  1940      1           25       35\n",
       "4  Bear-AG-29  1940      1           13       36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'ship_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-0752ea156b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malready_entered_pages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mto_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpages_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malready_entered_pages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ship_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"month\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"page_number\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mto_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_add\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_add\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"page_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_8/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7944\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7946\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   7947\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7948\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_8/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m ) -> \"DataFrame\":\n\u001b[0;32m---> 74\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_8/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_8/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_8/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ship_name'"
     ]
    }
   ],
   "source": [
    "pages_df = tile_df[columns].drop_duplicates()\n",
    "\n",
    "already_entered_pages = pd.read_sql(\"pages\",engine)\n",
    "\n",
    "display.display(already_entered_pages.head())\n",
    "\n",
    "columns = [\"ship_name\",\"year\",\"month\",\"page_number\"]\n",
    "to_add = pages_df.merge(already_entered_pages,how=\"left\",on=columns)\n",
    "to_add = to_add[to_add[\"page_id\"].isna()]\n",
    "\n",
    "display.display(to_add)\n",
    "to_add[columns].to_sql(\"pages\",engine,if_exists=\"append\",index=False)\n",
    "\n",
    "all_pages = pd.read_sql(\"pages\",engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ink pixels are fairly bright (barely indistinguishable from paper pixels), and so we set our threshold for paper/ink fairly high. However, for a full character, there should be at least some dark pixels, i.e. the bright ink pixels are on the boundary between ink and paper. The interior of the character will be dark. So if Tesseract finds a character where every pixel is bright, we should be worried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(all_characters[\"max_darkness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "all_characters.plot.scatter(x=\"max_darkness\",y=\"area\",ax=ax)\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = all_characters[\"max_darkness\"] <= 170\n",
    "all_characters[m].sort_values(\"max_darkness\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of these characters are completely suspect, but Tesseract still feels fairly confident. Also, for most the area seems far too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(all_characters,6771)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play around with the threshold and see that requiring max_darkness <= 170 seems reasonable. (Might be nice to create a more automated filter in the future.) From below, we see that this filters out about 7% of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darkness_mask = all_characters[\"max_darkness\"] <= 170\n",
    "s1 = all_characters[darkness_mask].shape[0]\n",
    "s2 = all_characters.shape[0]\n",
    "print(s1/s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, characters which are way too big or small need to be examined. Filtering out impossibly small characters is straightforward. However, it is a bit more complicated with bigger characters. These often include the correct character plus a whole bunch more. We will filter them out for now, since all of this is going to be fed into the autoencoder which benefits from characters being as similar as possible. However, we will want to feed these \"characters\" through the website to have the bounding boxes correct, whereas we can just drop the overly small characters completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_characters = all_characters[darkness_mask]\n",
    "filtered_characters.sort_values(\"area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(all_characters,14674)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "bins = range(filtered_characters[\"area\"].min(),filtered_characters[\"area\"].max(),1000)\n",
    "filtered_characters[\"area\"].hist(bins=bins)\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = filtered_characters[\"area\"] <= 10000\n",
    "filtered_characters[m].sort_values(\"area\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(all_characters,24215)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An upper bound of 1000 for area seems good, but we could probably reduce it a bit too if we wanted to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = filtered_characters[\"area\"] > 2000\n",
    "filtered_characters[m].sort_values(\"area\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounding box below is definitely too small, but the character is correct. So we may need to follow up on such bounding boxes in the website. (Also check to see if we have overlapping bounding boxes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(all_characters,12152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = filtered_characters[\"area\"] <= 10000\n",
    "m2 = filtered_characters[\"area\"] >= 2000\n",
    "\n",
    "double_filtered_characters = filtered_characters[m1&m2]\n",
    "s1 = double_filtered_characters.shape[0]\n",
    "s2 = all_characters.shape[0]\n",
    "print(s1/s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(character_df,tiles):\n",
    "    train_df,test_df = train_test_split(character_df, test_size=0.25, random_state=0)\n",
    "\n",
    "    train_images = tiles[train_df.index]\n",
    "    # we no longer need this index wrt to the original dataframe\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    test_images = tiles[test_df.index]\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    train_images = train_images / 255\n",
    "    test_images = test_images / 255\n",
    "\n",
    "    # sanity check\n",
    "    assert np.max(train_images) == 1\n",
    "    return train_images,test_images,train_df,test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,test_images,train_df,test_df = split(double_filtered_characters,tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display(train_df.iloc[0])\n",
    "_ = plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is taken from https://www.tensorflow.org/tutorials/generative/cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                # No activation\n",
    "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "                tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "\n",
    "    # not sure why the following had to be added in\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, epoch, test_sample):\n",
    "    mean, logvar = model.encode(test_sample)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    predictions = model.sample(z)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "#     plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_images,test_images,model_name,epochs):\n",
    "    # set the dimensionality of the latent space to a plane for visualization later\n",
    "    latent_dim = 2\n",
    "    num_examples_to_generate = 16\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_size = train_images.shape[0]\n",
    "    test_size = test_images.shape[0]\n",
    "\n",
    "\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
    "                     .shuffle(train_size).batch(batch_size))\n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "                    .shuffle(test_size).batch(batch_size))\n",
    "\n",
    "    assert batch_size >= num_examples_to_generate\n",
    "    for test_batch in test_dataset.take(1):\n",
    "        test_sample = test_batch[0:num_examples_to_generate, :, :, :]\n",
    "    \n",
    "    # keeping the random vector constant for generation (prediction) so\n",
    "    # it will be easier to see the improvement.\n",
    "    random_vector_for_generation = tf.random.normal(\n",
    "        shape=[num_examples_to_generate, latent_dim])\n",
    "    model = CVAE(latent_dim)\n",
    "\n",
    "    generate_images(model, 0, test_sample)\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        for train_x in train_dataset:\n",
    "            train_step(model, train_x, optimizer)\n",
    "        end_time = time.time()\n",
    "\n",
    "        loss = tf.keras.metrics.Mean()\n",
    "        for test_x in test_dataset:\n",
    "            loss(compute_loss(model, test_x))\n",
    "        elbo = -loss.result()\n",
    "\n",
    "\n",
    "        display.clear_output(wait=False)\n",
    "        print('Epoch: {}, Test set ELBO: {},time elapse for current epoch: {}'\n",
    "            .format(epoch, elbo,end_time - start_time))\n",
    "        generate_and_save_images(model, epoch, test_sample)\n",
    "        \n",
    "        \n",
    "            \n",
    "    return model\n",
    "\n",
    "def load_or_train_model(train_images,test_images,model_name,epochs):\n",
    "    \"\"\"\n",
    "    load a previously trained model. If no such model exists, train one\n",
    "    \"\"\"\n",
    "    weights_file = f\"{directory}weights_{model_name}\"\n",
    "    \n",
    "    if not os.path.exists(weights_file+\".index\"):\n",
    "        model = train_model(train_images,test_images,model_name,epochs)\n",
    "        model.save_weights(weights_file)\n",
    "    else:\n",
    "        print(\"loading\")\n",
    "        latent_dim = 2\n",
    "        num_examples_to_generate = 16\n",
    "        batch_size = 64\n",
    "        \n",
    "        model = CVAE(latent_dim)\n",
    "        model.load_weights(weights_file)\n",
    "        \n",
    "        # show how the loaded model is doing\n",
    "        test_size = test_images.shape[0]\n",
    "        test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "                    .shuffle(test_size).batch(batch_size))\n",
    "        for test_batch in test_dataset.take(1):\n",
    "            test_sample = test_batch[0:num_examples_to_generate, :, :, :]\n",
    "            \n",
    "        generate_images(model, 0, test_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very biased data set, so we'll check to see if reweighting the training and test set improves things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_model = load_or_train_model(train_images,test_images,\"base\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our CVAE, we will take a test example and see how well the autoencoder corrects the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = double_filtered_characters.groupby(\"character\").sample(n=150,replace=True)\n",
    "resampled_train_images,resampled_test_images,resampled_train_df,resampled_test_df = split(resampled,tiles)\n",
    "resampled_model = load_or_train_model(resampled_train_images,resampled_test_images,\"resampled\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could always increase the sameple size for the reweighted train/test set, but the initial results shown below are not promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df[\"character\"] == \"8\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df[\"character\"] == \"N\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 24\n",
    "\n",
    "print(test_df.loc[index,\"character\"])\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(test_images[index,:,:,0])\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "a,b = base_model.encode(test_images[index:index+1,:,:,:])\n",
    "z = base_model.reparameterize(a,b)\n",
    "x = base_model.decode(z,apply_sigmoid=True)\n",
    "ax.imshow(x[0,:,:,0])\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "a,b = resampled_model.encode(test_images[index:index+1,:,:,:])\n",
    "z = resampled_model.reparameterize(a,b)\n",
    "x = resampled_model.decode(z,apply_sigmoid=True)\n",
    "ax.imshow(x[0,:,:,0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well is our CVAE able to separate the characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characters_to_latent(df,images,model):\n",
    "    assert isinstance(df,pd.DataFrame)\n",
    "    mu = []\n",
    "    sigma = []\n",
    "\n",
    "    for index in range(images.shape[0]):\n",
    "        print(index,end=\"\\r\")\n",
    "\n",
    "        original = images[index:index+1,:,:,:]\n",
    "\n",
    "        a,b = model.encode(original)\n",
    "        z = model.reparameterize(a,b)\n",
    "\n",
    "        mu.append(float(z[0][0]))\n",
    "        sigma.append(float(z[0][1]))\n",
    "\n",
    "    df2 = pd.DataFrame({\"mu\":mu,\"sigma\":sigma})\n",
    "    return pd.concat([df,df2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_latent_df = characters_to_latent(resampled_test_df,resampled_test_images,resampled_model)\n",
    "latent_df = characters_to_latent(test_df,test_images,base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick test, how well does our CVAE differentiate numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = [chr(ord('0') + i) for i in range(10)]\n",
    "m = latent_df[\"character\"].isin(of_interest)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='mu', y='sigma', hue='character', data=latent_df[m])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(10, weights='uniform')\n",
    "clf.fit(latent_df[[\"mu\",\"sigma\"]], latent_df[\"character\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df = latent_df.groupby(\"character\")[[\"mu\",\"sigma\"]].median().reset_index()\n",
    "ideal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code seems to be easier to follow if everything is in dense format (as opposed to sparse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "ideal_as_1hot = enc.fit_transform(ideal_df[[\"character\"]]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = clf.predict_proba(ideal_df[[\"mu\",\"sigma\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely = np.multiply(ideal_as_1hot,probabilities)\n",
    "ideal_df[\"likelyhood\"] = np.amax(most_likely,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which characters are we most confident about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df.sort_values(\"likelyhood\").tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_character(model,x,y):\n",
    "    s = np.array([x,y])\n",
    "    s = s.reshape(1,2)\n",
    "    \n",
    "    remapped = model.decode(s,apply_sigmoid=True)\n",
    "\n",
    "    a = (np.array(remapped)*255).astype(np.uint8)\n",
    "    b = a.reshape((a.shape[1],a.shape[2]))\n",
    "    ret2,th2 = cv2.threshold(b,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    th2 = 255 - th2\n",
    "    return th2\n",
    "\n",
    "def generate_ideal(model,ideal_df,ch):\n",
    "    r = ideal_df[ideal_df[\"character\"] == ch]\n",
    "    return generate_character(model,r[\"mu\"],r[\"sigma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CVAE handles '4' very well. We have identified some characters which according to our model are 4 but Tessearct thinks differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(generate_ideal(base_model,ideal_df,\"T\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at some of the raw characters. Start with characters we think are likely to be 4, but tesseract doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df[ideal_df[\"character\"] == \"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = enc.transform(latent_df[[\"character\"]]).todense()\n",
    "probabilities = clf.predict_proba(latent_df[[\"mu\",\"sigma\"]])\n",
    "latent_df[\"p\"] = probabilities[:,15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when Tessearct thinks a character is 4, 77% of the time we agree. However, when Tessearct thinks a character is k, 60% we think the character is actaully 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.groupby(\"character\")[\"p\"].mean().to_frame().sort_values(\"p\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = results_df[\"character\"] == \"k\"\n",
    "results_df[m].sort_values(\"p\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"/home/ggdhines/bear/Bear-AG-29-1940-01-39_ocr_ready.png\",0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = results_df[\"fname\"] == \"Bear-AG-29-1940-01-39_ocr_ready.png\"\n",
    "m2  = results_df[\"max prob\"] >= 0.8\n",
    "results_df[m1].sort_values(\"top\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our classifcation is better than Tessearct's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(results_df,4159)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Tesseract is actually often mislabelling '4's as 'k's. Note that the above 4 is missing a fair bit since it overlaps with a grid line, yet we still estimate the probability of it being a '4' to be 83%/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size vs. likelyhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = results_df.groupby(\"character\").size().reset_index()\n",
    "df3 = ideal_df.merge(df2,on=\"character\")\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "df3.plot.scatter(x=0,y=\"likelyhood\",ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is surprisingly little correlation between Tessearct's confidence and ours\n",
    "### (Remeber that we filtered to only includes tiles which Tesseract had a confidence of at least 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual = enc.transform(latent_df[[\"character\"]]).todense()\n",
    "probabilities = clf.predict_proba(latent_df[[\"mu\",\"sigma\"]])\n",
    "latent_df[\"max prob\"] = np.amax(probabilities,axis=1)\n",
    "\n",
    "_ = latent_df.plot.scatter(x=\"confidence\",y=\"max prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Tiles are we least certain about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely = np.argmax(probabilities,axis=1)\n",
    "inverse = [c[-1] for c in enc.get_feature_names()]\n",
    "most_likely = [inverse[i] for i in most_likely]\n",
    "latent_df[\"most_likely\"] = most_likely\n",
    "\n",
    "latent_df.sort_values(\"max prob\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here Tesseract is completely correct and we're not. So what happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df.to_csv(\"/home/ggdhines/PycharmProjects/historical-transcriptions/dataframes/latent1.cvs\")\n",
    "ideal_df.to_csv(\"/home/ggdhines/PycharmProjects/historical-transcriptions/dataframes/ideal.cvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_character(latent_df,582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df[ideal_df[\"character\"].isin([\"N\",\"\\\"\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = [\"N\",\"\\\"\"]\n",
    "m = latent_df[\"character\"].isin(of_interest)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='mu', y='sigma', hue='character', data=latent_df[m])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ideal_df.loc[ideal_df[\"character\"] == \"2\",[\"mu\",\"sigma\"]].values[0]\n",
    "\n",
    "m = results_df[\"character\"] == \"2\"\n",
    "df2 = results_df[m]\n",
    "d = df2[[\"mu\",\"sigma\"]].values\n",
    "\n",
    "distance = np.sqrt((d[:,0]-u[0])**2 + (d[:,1]-u[1])**2)\n",
    "df2[\"distance\"] = distance\n",
    "\n",
    "df2.sort_values(\"distance\",ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(test_images[1614,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So why is Z so bad? Relatively few tiles. How good are they?\n",
    "\n",
    "* with less than 100 tiles, knn will underestimate the likelyhood of Z. But the CVAE seems to have a hard differentiating Z from E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = results_df[\"character\"] == \"Z\"\n",
    "results_df[m].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(test_images[3627,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two tiles. Both have low max probability. Tesseract identifies the first as being for a character which we have a high confidence for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_df.sort_values(\"likelyhood\",ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = latent_df[\"character\"] == \"%\"\n",
    "latent_df[m].sort_values(\"max prob\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(test_images[3066,:,:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
